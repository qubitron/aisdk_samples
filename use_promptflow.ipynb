{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config variables\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "credential=DefaultAzureCredential()\n",
    "resource_group_name = \"dantaylo_sdktest_rg3\"\n",
    "hub_name = \"dantaylo_sdktest_hub3\"\n",
    "project_name = \"dantaylo-sdktest-project3\"\n",
    "location = \"eastus\"\n",
    "subscription_id = \"597966d1-829f-417e-9950-8189061ec09c\"\n",
    "aiservices_resource_name = \"dantaylo_sdktest_aiservices3\"\n",
    "aisearch_resource_name = \"dantaylo-sdktest-aisearch3\"\n",
    "aisearch_index_name = \"dantaylo_aisearch_index\"\n",
    "azure_openai_chat_model = \"gpt-35-turbo\"\n",
    "azure_openai_embedding_model = \"text-embedding-ada-002\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build index\n",
    "from config import resource_group_name, project_name, subscription_id, credential,azure_openai_embedding_model_name, aisearch_index_name\n",
    "from promptflow.rag.resources import LocalSource, AzureAISearchConfig, EmbeddingsModelConfig, ConnectionConfig\n",
    "from promptflow.rag import build_index\n",
    "\n",
    "  # Use the same index name when registering the index in AI Studio\n",
    "index_path = build_index(\n",
    "    name=aisearch_index_name,  # name of your index\n",
    "    vector_store=\"azure_ai_search\",  # the type of vector store\n",
    "    embeddings_model_config=EmbeddingsModelConfig(\n",
    "        model_name=azure_openai_embedding_model_name, # the name of the model\n",
    "        deployment_name=azure_openai_embedding_model_name,\n",
    "        connection_config= ConnectionConfig(\n",
    "            subscription=subscription_id,\n",
    "            resource_group=resource_group_name,\n",
    "            workspace=project_name,\n",
    "            connection_name=\"azure_aiservices_connection\"\n",
    "        )\n",
    "    ),\n",
    "    input_source=LocalSource(input_data=\"./data/*.md\"),  # the location of your file/folders\n",
    "    index_config=AzureAISearchConfig(\n",
    "            ai_search_index_name=index_name, # the name of the index store inside the azure ai search service\n",
    "            ai_search_connection_config=ConnectionConfig(\n",
    "            subscription=subscription_id,\n",
    "            resource_group=resource_group_name,\n",
    "            workspace=project_name,\n",
    "            connection_name=\"azure_aisearch_connection\"\n",
    "        ) # this is so that you do not need the environment variables\n",
    "    ),\n",
    "    tokens_per_chunk = 800, # Optional field - Maximum number of tokens per chunk\n",
    "    token_overlap_across_chunks = 0, # Optional field - Number of tokens to overlap between chunks\n",
    ")\n",
    "\n",
    "from promptflow.rag import get_langchain_retriever_from_index\n",
    "\n",
    "# Call the index\n",
    "retriever=get_langchain_retriever_from_index(ml_index.path)\n",
    "retriever.get_relevant_documents(\"which tent is the most waterproof?\")\n",
    "\n",
    "# register the index so that it shows up in the project\n",
    "from azure.ai.ml import MLClient\n",
    "client.indexes.create_or_update(Index(name=index_name, path=index_path))\n",
    "client = MLClient(workspace_name=project_name, \n",
    "    resource_group_name=resource_group_name, \n",
    "    subscription_id=subscription_id, \n",
    "    credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from promptflow.core import tool, Prompty\n",
    "\n",
    "# load and execute prompty\n",
    "path_to_prompty =  \"basic_chat.prompty\"\n",
    "flow = Prompty.load(path_to_prompty)\n",
    "\n",
    "# execute the flow as function\n",
    "result = flow(question=question)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from promptflow.core import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Initialize Azure OpenAI Connection\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    ")\n",
    "\n",
    "# Run built-in evaluator on a single input row\n",
    "from promptflow.evals.evaluators import RelevanceEvaluator\n",
    "\n",
    "relevance_eval = RelevanceEvaluator(model_config)\n",
    "relevance_score = relevance_eval(\n",
    "    answer=\"The Alpine Explorer Tent is the most waterproof.\",\n",
    "    context=\"From the our product list,\"\n",
    "    \" the alpine explorer tent is the most waterproof.\"\n",
    "    \" The Adventure Dining Table has higher weight.\",\n",
    "    question=\"Which tent is the most waterproof?\",\n",
    ")\n",
    "# relevance_score: 5.0\n",
    "\n",
    "# custom evaluator\n",
    "from promptflow.client import load_flow\n",
    "\n",
    "# load apology evaluatorfrom prompty\n",
    "apology_eval = load_flow(source=\"apology.prompty\", model={\"configuration\": model_config})\n",
    "\n",
    "result = apology_eval(\n",
    "    question=\"What is the capital of France?\", answer=\"Paris\"\n",
    ")\n",
    "\n",
    "# result: {'apology': 0}\n",
    "\n",
    "\n",
    "from promptflow.evals.evaluate import evaluate\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_eval,\n",
    "        \"violence\": violence_eval,\n",
    "        \"answer_length\": answer_length,\n",
    "        \"apology\": apology_eval\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"ground_truth\": \"${data.truth}\"\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
